import { Injectable, Inject, Logger, OnModuleInit } from '@nestjs/common';
import { AiModuleOptions } from './ai.module';
import { GenerationOptions } from './interfaces/ai.interfaces';
import { Ollama } from '@langchain/ollama';

function isErrorWithMessage(error: unknown): error is { message: string } {
    return (
        typeof error === 'object' &&
        error !== null &&
        'message' in error &&
        typeof (error as { message: unknown }).message === 'string'
    );
}

@Injectable()
export class AiService implements OnModuleInit {
    private readonly logger = new Logger(AiService.name);
    private isConnected = false;
    private config: AiModuleOptions['config'];
    private ollama: Ollama;

    constructor(@Inject('AI_OPTIONS') private readonly options: AiModuleOptions) {
        this.config = options.config;
        
        // LangChain Ollama ì´ˆê¸°í™”
        this.ollama = new Ollama({
            baseUrl: this.config.url || 'http://localhost:11434',
            model: this.config.model || 'gemma3n:e4b',
            temperature: 0.7,
        });
    }

    async onModuleInit() {
        await this.initializeConnection();
    }

    // ================== ì—°ê²° ê´€ë¦¬ ==================

    private async initializeConnection(): Promise<void> {
        try {
            // Ollama ì„œë²„ ì—°ê²° í…ŒìŠ¤íŠ¸
            const response = await fetch(`${this.config.url}/api/tags`);
            if (!response.ok) {
                throw new Error(`Ollama connection failed: ${response.status}`);
            }

            // ëª¨ë¸ ì¡´ì¬ í™•ì¸
            const models = await response.json() as { models: { name: string }[] };
            const hasModel = models.models.some((m) => m.name.includes(this.config.model!));

            if (!hasModel) {
                this.logger.warn(`Model ${this.config.model} not found, attempting to pull...`);
                await this.pullModel();
            }

            this.isConnected = true;
            this.logger.log(`ğŸ¤– AI Service initialized - Model: ${this.config.model}`);
        } catch (error: unknown) {
            this.logger.warn(`AI service initialization failed, using fallback mode: ${this.getErrorMessage(error)}`);
            this.isConnected = false;
        }
    }

    public async pullModel(): Promise<void> {
        try {
            const response = await fetch(`${this.config.url}/api/pull`, {
                method: 'POST',
                headers: { 'Content-Type': 'application/json' },
                body: JSON.stringify({ name: this.config.model }),
            });

            if (!response.ok) {
                throw new Error(`Failed to pull model ${this.config.model}`);
            }
            
            this.logger.log(`âœ… Model ${this.config.model} pulled successfully`);
        } catch (error) {
            this.logger.warn(`Failed to pull model: ${this.getErrorMessage(error)}`);
        }
    }

    // ================== í…ìŠ¤íŠ¸ ìƒì„± ==================

    async generateResponse(prompt: string, options?: GenerationOptions): Promise<string> {
        try {
            // JSON ì‘ë‹µ ì „ìš© í”„ë¡¬í”„íŠ¸ ê°•í™”
            const enhancedPrompt = this.enhancePromptForJson(prompt);
            
            // ì˜µì…˜ì´ ìˆìœ¼ë©´ temperature ë™ì  ì„¤ì •
            if (options?.temperature !== undefined) {
                this.ollama.temperature = options.temperature;
            }
            
            // LangChain Ollama ì‚¬ìš©
            const response = await this.ollama.invoke(enhancedPrompt);
            return response;
        } catch (error) {
            this.logger.warn(`AI generation failed: ${this.getErrorMessage(error)}`);
            // í´ë°± ì‘ë‹µ ì‚¬ìš©
            return this.generateFallbackContent(prompt);
        }
    }

    /**
     * JSON ì‘ë‹µì„ ìœ„í•œ í”„ë¡¬í”„íŠ¸ ê°•í™”
     */
    private enhancePromptForJson(prompt: string): string {
        // JSON ì‘ë‹µì´ í•„ìš”í•œ í”„ë¡¬í”„íŠ¸ì¸ì§€ í™•ì¸
        if (prompt.includes('JSON') || prompt.includes('json') || prompt.includes('{')) {
            return `System: You must respond with ONLY valid JSON. No markdown code blocks. No explanations. No \`\`\`json or \`\`\`. Just the JSON object.

${prompt}

Remember: ONLY JSON output. NO markdown formatting.`;
        }
        
        // URL ì œì•½ì‚¬í•­ ì¶”ê°€
        return `${prompt}

ì¤‘ìš”í•œ ì œì•½ì‚¬í•­:
- ì–´ë–¤ URL, ë§í¬, ì›¹ ì£¼ì†Œë„ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”
- ìœ íŠœë¸Œ ë§í¬ë‚˜ ì™¸ë¶€ ì‚¬ì´íŠ¸ ë§í¬ë¥¼ ìƒì„±í•˜ì§€ ë§ˆì„¸ìš”
- ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ë§í¬ë¥¼ ë§Œë“¤ì–´ë‚´ì§€ ë§ˆì„¸ìš”
- í…ìŠ¤íŠ¸ ê¸°ë°˜ì˜ ë ˆì‹œí”¼ ì •ë³´ì—ë§Œ ì§‘ì¤‘í•˜ì„¸ìš”`;
    }

    async *streamText(prompt: string, options?: GenerationOptions): AsyncIterable<{ content: string; done: boolean }> {
        try {
            const enhancedPrompt = this.enhancePromptForJson(prompt);
            
            if (options?.temperature !== undefined) {
                this.ollama.temperature = options.temperature;
            }
            
            // LangChainì˜ ìŠ¤íŠ¸ë¦¼ ê¸°ëŠ¥ ì‚¬ìš©
            const stream = await this.ollama.stream(enhancedPrompt);
            
            for await (const chunk of stream) {
                yield {
                    content: chunk,
                    done: false
                };
            }
            
            yield {
                content: '',
                done: true
            };
        } catch (error) {
            this.logger.error(`Stream generation failed: ${this.getErrorMessage(error)}`);
            yield* this.getFallbackStream(prompt);
        }
    }

    // ================== í´ë°± ì‘ë‹µ ==================

    private async *getFallbackStream(prompt: string): AsyncIterable<{ content: string; done: boolean }> {
        const response = this.generateFallbackContent(prompt);
        const words = response.split(' ');

        for (let i = 0; i < words.length; i++) {
            await new Promise(resolve => setTimeout(resolve, 50));
            yield {
                content: words[i] + ' ',
                done: i === words.length - 1,
            };
        }
    }

    private generateFallbackContent(prompt: string): string {
        const lowerPrompt = prompt.toLowerCase();

        if (lowerPrompt.includes('ì•ˆë…•') || lowerPrompt.includes('hello')) {
            return 'ì•ˆë…•í•˜ì„¸ìš”! AI ìš”ë¦¬ ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ì–´ë–¤ ìš”ë¦¬ë¥¼ ë„ì™€ë“œë¦´ê¹Œìš”? ğŸ³';
        }

        if (lowerPrompt.includes('ìš”ë¦¬') || lowerPrompt.includes('ë ˆì‹œí”¼')) {
            return 'ìš”ë¦¬ì— ëŒ€í•´ ê¶ê¸ˆí•˜ì‹  ê²ƒì´ ìˆìœ¼ì‹œêµ°ìš”! êµ¬ì²´ì ìœ¼ë¡œ ì–´ë–¤ ìš”ë¦¬ë‚˜ ì¬ë£Œì— ëŒ€í•´ ì•Œê³  ì‹¶ìœ¼ì‹ ê°€ìš”?';
        }

        if (lowerPrompt.includes('ìŒì‹') || lowerPrompt.includes('ì¬ë£Œ')) {
            return 'ìŒì‹ê³¼ ì¬ë£Œì— ê´€í•œ ì§ˆë¬¸ì´ì‹œë„¤ìš”! ì–´ë–¤ ì¢…ë¥˜ì˜ ìš”ë¦¬ë²•ì´ë‚˜ ì¬ë£Œ ì¡°í•©ì— ëŒ€í•´ ë„ì›€ì´ í•„ìš”í•˜ì‹ ê°€ìš”?';
        }

        if (lowerPrompt.includes('ê³ ë§ˆì›Œ') || lowerPrompt.includes('thank')) {
            return 'ì²œë§Œì—ìš”! ë§›ìˆëŠ” ìš”ë¦¬ ë˜ì„¸ìš”! ë‹¤ë¥¸ ê¶ê¸ˆí•œ ê²ƒì´ ìˆìœ¼ì‹œë©´ ì–¸ì œë“  ë§ì”€í•´ì£¼ì„¸ìš”. ğŸ˜Š';
        }

        return 'ì£„ì†¡í•©ë‹ˆë‹¤. í˜„ì¬ AI ì„œë¹„ìŠ¤ì— ì¼ì‹œì ì¸ ë¬¸ì œê°€ ìˆì–´ ì ì ˆí•œ ì‘ë‹µì„ ë“œë¦¬ê¸° ì–´ë µìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.';
    }

    private getErrorMessage(error: unknown): string {
        if (error instanceof Error) {
            return error.message;
        }
        if (typeof error === 'string') {
            return error;
        }
        if (isErrorWithMessage(error)) {
            return error.message;
        }
        return 'An unknown error occurred';
    }

    // ================== ìƒíƒœ ê´€ë¦¬ ==================

    async getStatus(): Promise<{
        isConnected: boolean;
        model: string | undefined;
        config: { url?: string; timeout?: number };
    }> {
        return {
            isConnected: this.isConnected,
            model: this.config.model,
            config: {
                url: this.config.url,
                timeout: this.config.timeout,
            },
        };
    }

    async reconnect(): Promise<void> {
        this.logger.log('Attempting to reconnect AI service...');
        await this.initializeConnection();
    }
}